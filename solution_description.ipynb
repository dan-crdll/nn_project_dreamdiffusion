{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbbccd30502d097",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Re-implementation of the method proposed in ''DreamDiffusion: Generating High-Quality Images from Brain EEG Signals'' by Y. Bai, X. Wang et al.\n",
    "*By Daniele Santino Cardullo | 2127806 | cardullo.2127806@studenti.uniroma1.it*\n",
    "\n",
    "*original work: [DreamDiffusion (arXiv)](https://arxiv.org/abs/2306.16934)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec826938",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "## Description of the implemented models and architectures\n",
    "\n",
    "## How to run my code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f69864",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "### Downloads\n",
    "Here are some links to download the datasets and pretrained models (extract them in the same folder where this notebook is):\n",
    "- pretrained_models folder: \n",
    "- datasets folder:\n",
    "\n",
    "These are needed to run the `Using the already pretrained model` block you'll find at the end of every step, this block is intended to be standalone hence can be run without running the precedings blocks. \n",
    "\n",
    "### First stage: Pretraining\n",
    "The first stage in this work is the construction and pretraining of the *masked autoencoder for EEG signals*.\n",
    "The datasets used for pretraining are selected from the MOABB family, a set of workbench for brain-computer interfaces development. In this implementation the model has been scaled with respect to the original work, still achieving good results both in terms of pretraining performances and final results.\n",
    "\n",
    "The first step for pretraining is to load the datasets that will be used, in this case 5 subjects are selected from `BI2015b` dataset and 2 subjects are selected from `Lee2019_SSVEP`. The data is preprocessed in the dataset creation, filtering the signals between 5-95 Hz and limiting their timestep to 500 ticks; also each signal has been padded to have 128 channels by repeating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lightning.pytorch import seed_everything\n",
    "from source.datasets.pretraining_dataset import EegPretrainDataset\n",
    "from hydra import compose, initialize\n",
    "\n",
    "with initialize(version_base=None, config_path=\"./\", job_name=\"dream_diffusion\"):\n",
    "    cfg = compose(config_name=\"default_config\")\n",
    "\n",
    "# Seeding for repeatability\n",
    "generator = torch.manual_seed(cfg.pretrain.seed)\n",
    "seed_everything(cfg.pretrain.seed, workers=True)\n",
    "\n",
    "ds = EegPretrainDataset(\n",
    "    dim=cfg.pretrain.time_dimension, \n",
    "    train_perc=cfg.pretrain.train_perc)  # Preparation of the dataset (loading of data and preprocessing)\n",
    "\n",
    "train_ds, test_ds = ds()    # Train and Test splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a0918",
   "metadata": {},
   "source": [
    "After having loaded the dataset it is possible to create the dataloaders for training and testing, in order to automatically create batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=cfg.pretrain.bsz, generator=generator, num_workers=11)\n",
    "test_dl = DataLoader(test_ds, batch_size=cfg.pretrain.bsz, generator=generator, num_workers=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d72cd",
   "metadata": {},
   "source": [
    "After having loaded the data, we can finally instance the MAE model, along with the trainer. In this project Lightning has been used to enhance the training process limiting all the boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.eeg_mae.masked_decoder import MaskedDecoder\n",
    "from source.eeg_mae.masked_encoder import MaskedEncoder\n",
    "from source.eeg_mae.encoder_config import EncoderConfig\n",
    "from source.eeg_mae.eeg_autoencoder import EegAutoEncoder\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "config = EncoderConfig(time_dim=cfg.pretrain.time_dimension, \n",
    "                       token_num=cfg.pretrain.time_dimension//4, \n",
    "                       embed_dim=cfg.mae.embed_dim, \n",
    "                       encoder_depth=cfg.mae.encoder_depth, \n",
    "                       encoder_heads=cfg.mae.encoder_heads, \n",
    "                       mask_perc=cfg.mae.mask_perc)\n",
    "encoder = MaskedEncoder(config)\n",
    "decoder = MaskedDecoder(cfg.pretrain.time_dimension, \n",
    "                        cfg.pretrain.time_dimension//4, \n",
    "                        cfg.mae.channels, \n",
    "                        cfg.mae.embed_dim,\n",
    "                        cfg.mae.decoder_depth, \n",
    "                        cfg.mae.decoder_heads)\n",
    "\n",
    "model = EegAutoEncoder(encoder, decoder, learning_rate=cfg.pretrain.lr)\n",
    "trainer = Trainer(\n",
    "    max_epochs=cfg.pretrain.epochs,\n",
    "    log_every_n_steps=1,\n",
    "    deterministic=True,\n",
    "    enable_checkpointing=True, \n",
    "    gradient_clip_val=0.5,\n",
    "    check_val_every_n_epoch=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4223ec",
   "metadata": {},
   "source": [
    "And now it is possible to start the pretraining process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831fc084",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dl, val_dataloaders=train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58373370",
   "metadata": {},
   "source": [
    "#### Using the already pretrained model\n",
    "If you have downloaded the `pretrained_models` and `datasets` folders from the link in `Downloads` section, it is possible to directly load the already pretrained model and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571d53de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset Loading\n",
    "\"\"\"\n",
    "import torch\n",
    "from lightning.pytorch import seed_everything\n",
    "from source.datasets.pretraining_dataset import EegPretrainDataset\n",
    "from hydra import compose, initialize\n",
    "from source.eeg_mae.masked_decoder import MaskedDecoder\n",
    "from source.eeg_mae.masked_encoder import MaskedEncoder\n",
    "from source.eeg_mae.encoder_config import EncoderConfig\n",
    "from source.eeg_mae.eeg_autoencoder import EegAutoEncoder\n",
    "from lightning.pytorch import Trainer\n",
    "from hydra import compose, initialize\n",
    "\n",
    "with initialize(version_base=None, config_path=\"./\", job_name=\"dream_diffusion\"):\n",
    "    cfg = compose(config_name=\"default_config\")\n",
    "\n",
    "generator = torch.manual_seed(cfg.pretrain.seed)\n",
    "seed_everything(cfg.pretrain.seed, workers=True)\n",
    "\n",
    "ds = EegPretrainDataset(\n",
    "    dim=cfg.pretrain.time_dimension, \n",
    "    train_perc=cfg.pretrain.train_perc, ds_path='./datasets/pretrain_dataset.pth')\n",
    "\n",
    "train_ds, test_ds = ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Loading\n",
    "\"\"\"\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=cfg.pretrain.bsz, generator=generator, num_workers=11)\n",
    "test_dl = DataLoader(test_ds, batch_size=cfg.pretrain.bsz, generator=generator, num_workers=11)\n",
    "\n",
    "config = EncoderConfig(time_dim=cfg.pretrain.time_dimension, \n",
    "                       token_num=cfg.pretrain.time_dimension//4, \n",
    "                       embed_dim=cfg.mae.embed_dim, \n",
    "                       encoder_depth=cfg.mae.encoder_depth, \n",
    "                       encoder_heads=cfg.mae.encoder_heads, \n",
    "                       mask_perc=cfg.mae.mask_perc)\n",
    "encoder = MaskedEncoder(config)\n",
    "decoder = MaskedDecoder(cfg.pretrain.time_dimension, \n",
    "                        cfg.pretrain.time_dimension//4, \n",
    "                        cfg.mae.channels, \n",
    "                        cfg.mae.embed_dim,\n",
    "                        cfg.mae.decoder_depth, \n",
    "                        cfg.mae.decoder_heads)\n",
    "\n",
    "model = EegAutoEncoder(encoder, decoder, learning_rate=cfg.pretrain.lr).load_from_checkpoint('./pretrained_models/pretrained_mae.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786beb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Results\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch = next(iter(test_dl))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(batch)\n",
    "    \n",
    "fig, axes = plt.subplots(4, 2)\n",
    "\n",
    "for i in range(4):\n",
    "    axes[i, 0].plot(batch[i, :, :])\n",
    "    axes[i, 1].plot(prediction[i, :, :])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
