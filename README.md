 âš ï¸ Still a work in progress ï¸âš ï¸

### Re-implementation of the method proposed in ''DreamDiffusion: Generating High-Quality Images from Brain EEG Signals'' by Y. Bai, X. Wang et al.
*By Daniele Santino Cardullo | 2127806 | cardullo.2127806@studenti.uniroma1.it*

*original work: [DreamDiffusion (arXiv)](https://arxiv.org/abs/2306.16934)*

This work is part of the Neural Network Course Exam for academic year 2023 / 2024,
all the credits for the original work 
and publication go to the original authors.

#### Abstract
DreamDiffusion is a method for generating images directly from electroencephalogram signals. This is achieved by combinating different methodologies such as: self-supervised learning to learn meaningful and efficient latent representations for signals; latent diffusion generative model to generate high quality images; large language model to align signals embeddings with image-text ones.

#### Run The Code
To run the code create a virtual environment and install requirements, then take a look at `solution_description.ipynb`.

#### Directory Tree
<pre>
ğŸ“¦ nn_project_dreamdiffusion
â”œâ”€Â .gitignore
â”œâ”€Â README.md
â”œâ”€Â default_config.yaml
â”œâ”€Â requirements.txt
â”œâ”€Â solution_description.ipynb
â”œâ”€Â datasets
â”‚Â Â â”œâ”€Â finetune_images/
â”‚Â Â â”œâ”€Â finetune_dataset.pth
â”‚Â Â â””â”€Â pretrain_dataset.pth
â”œâ”€Â pretrained_models
â”‚Â Â â”œâ”€Â pretrained_mae.ckpt
â”‚Â Â â”œâ”€Â finetuned_eeg_encoder.pth
â”‚Â Â â”œâ”€Â finetuned_unet.pth
â”‚Â Â â”œâ”€Â finetuned_projector_tau.pth
â”‚Â Â â””â”€Â train_loss_mae.csv
â””â”€Â source
Â Â Â â”œâ”€Â datasets
Â Â Â â”‚Â Â â”œâ”€Â finetuning_dataset
Â Â Â â”‚Â Â â””â”€Â pretraining_dataset.py
Â Â Â â”œâ”€Â eeg_diffusion
Â Â Â â”‚Â Â â”œâ”€Â dream_diffusion.
Â Â Â â”‚Â Â â””â”€Â projector.py
Â Â Â â””â”€Â eeg_mae
Â Â Â Â Â Â â”œâ”€Â attention_block.py
Â Â Â Â Â Â â”œâ”€Â eeg_autoencoder.py
Â Â Â Â Â Â â”œâ”€Â encoder_config.py
Â Â Â Â Â Â â”œâ”€Â masked_decoder.py
Â Â Â Â Â Â â”œâ”€Â masked_encoder.py
Â Â Â Â Â Â â””â”€Â masked_loss.py
</pre>

#### Links